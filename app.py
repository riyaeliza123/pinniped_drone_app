import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'

import streamlit as st
import cv2
import tempfile
import gc
import supervision as sv
import inspect
import psutil
from collections import defaultdict
from math import cos, radians
import requests
import zipfile
import io

from scripts.detection import run_detection_from_local
from scripts.exif_utils import extract_gps_from_image, get_capture_date_time, get_location_name
from scripts.image_utils import limit_resolution_to_temp, progressive_resize_to_temp, compute_gsd
from scripts.clustering import compute_unique_counts
from scripts.summaries import display_and_download_summary
from scripts.s3_utils import (
    upload_to_s3_direct, 
    download_from_s3, 
    list_s3_files,
    delete_s3_folder,
    clean_entire_dropbox_extracts_folder
)

# Get S3 bucket name
try:
    S3_BUCKET = st.secrets["S3_BUCKET_NAME"]
except:
    S3_BUCKET = os.getenv("S3_BUCKET_NAME")
    if not S3_BUCKET:
        st.error("S3_BUCKET_NAME not configured. Please add it to .streamlit/secrets.toml")
        st.stop()

st.title("Pinniped Detection from Drone Imagery of Log Booms")

st.markdown("### ‚öôÔ∏è Detection Thresholds")

st.info("""
üí° **Tip:** Leave both at default values if the model is performing well on your images.
""")
conf_threshold = st.slider("Confidence threshold (%)", 0, 100, 15, step=5, help="Minimum confidence score (0-100%) for the model to consider a detection as valid. Lower values = more detections (including potential false positives). Higher values = fewer, more confident detections. **Default (15%)** works well for most cases. Increase if you see too many incorrect detections.")
overlap_threshold = st.slider("Overlap / NMS threshold (%)", 0, 100, 30, step=5, help= "Removes duplicate detections of the same object. Lower values = stricter filtering (fewer boxes). Higher values = more lenient filtering (keeps overlapping boxes). **Default (30%)** works well for most cases. Adjust only if you notice issues with duplicate or missing detections.")

st.markdown("### üì¶ Dropbox ZIP Link")
st.info("""
**How to share via Dropbox:**
1. Upload your images folder as a ZIP file to Dropbox. The size of the ZIP **must not exceed 2 GB**.
2. Right-click the ZIP ‚Üí Share ‚Üí Copy link.
3. Paste the link below and press Enter.
4. Click "Download and Process from Dropbox".

**Note:** The ZIP will be downloaded to S3, extracted, and processed from there.
""")

dropbox_url = st.text_input("Dropbox ZIP file link:", value="")

if not dropbox_url:
    st.info("üëÜ Enter a Dropbox ZIP link to begin")
    st.stop()

# Convert Dropbox share link to direct download link
if 'dropbox.com' not in dropbox_url:
    st.error("Please provide a valid Dropbox link")
    st.stop()

direct_url = dropbox_url.replace('dl=0', 'dl=1').replace('www.dropbox.com', 'dl.dropboxusercontent.com')

# Check if we already have results in session state
if 'summary_records' in st.session_state and len(st.session_state['summary_records']) > 0:
    # Display existing results without reprocessing
    st.success(f"‚úÖ Results ready! Processed {len(st.session_state['summary_records'])} images")
    
    st.markdown("### üìä Memory Usage")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Peak RAM", f"{st.session_state.get('peak_memory', 0):.1f} MB")
    with col2:
        st.metric("Final RAM", f"{st.session_state.get('final_memory', 0):.1f} MB")
    with col3:
        st.metric("RAM Increase", f"{st.session_state.get('memory_used', 0):.1f} MB")

    st.markdown("### üì∑ Annotated Images")
    for idx, ap in enumerate(st.session_state.get('annotated_paths', [])):
        try:
            if idx < len(st.session_state['summary_records']):
                filename = st.session_state['summary_records'][idx]['image_name']
            else:
                filename = f"Image {idx + 1}"
            
            st.image(cv2.cvtColor(cv2.imread(ap), cv2.COLOR_BGR2RGB))
            st.markdown(f"<p style='text-align: center; color: gray; font-size: 12px; margin-top: -15px;'>Img: {filename}</p>", unsafe_allow_html=True)
        except:
            st.image(ap)
            if idx < len(st.session_state['summary_records']):
                filename = st.session_state['summary_records'][idx]['image_name']
                st.markdown(f"<p style='text-align: center; color: gray; font-size: 12px; margin-top: -15px;'>Img: {filename}</p>", unsafe_allow_html=True)

    folder_summary_records = compute_unique_counts(
        st.session_state['grouped_coords'],
        st.session_state['max_counts']
    )
    display_and_download_summary(
        st.session_state['summary_records'],
        folder_summary_records,
        st.session_state['all_detections_records']
    )
    st.stop()

# Process Dropbox ZIP
image_files = []
s3_folder_key = None

if st.button("Download and Process from Dropbox"):
    try:
        # Clean S3 before starting
        with st.spinner("Cleaning S3 workspace..."):
            clean_entire_dropbox_extracts_folder(S3_BUCKET)
        
        import uuid
        session_id = uuid.uuid4().hex
        s3_folder_key = f"dropbox_extracts/{session_id}/"
        
        # Download ZIP file in chunks
        response = requests.get(direct_url, stream=True, timeout=60)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        
        # Check file size limit (2 GB)
        MAX_SIZE_BYTES = 2 * 1024 * 1024 * 1024  # 2 GB
        if total_size > MAX_SIZE_BYTES:
            st.error(f"‚ùå ZIP file is too large: {total_size / (1024*1024*1024):.2f} GB")
            st.warning(f"üì¶ Maximum allowed size: {MAX_SIZE_BYTES / (1024*1024*1024):.2f} GB")
            st.info("Please split your images into smaller ZIP files (< 2 GB each) and process them separately.")
            if s3_folder_key:
                delete_s3_folder(S3_BUCKET, s3_folder_key)
            st.stop()
        
        with st.spinner("Downloading ZIP file from Dropbox..."):
            downloaded = 0
            
            download_progress = st.progress(0)
            download_status = st.empty()
            
            # Download to memory
            zip_bytes = io.BytesIO()
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    zip_bytes.write(chunk)
                    downloaded += len(chunk)
                    
                    # Check size during download
                    if downloaded > MAX_SIZE_BYTES:
                        st.error(f"‚ùå ZIP file exceeded 2 GB limit during download")
                        st.info("Please split your images into smaller ZIP files (< 2 GB each) and process them separately.")
                        if s3_folder_key:
                            delete_s3_folder(S3_BUCKET, s3_folder_key)
                        st.stop()
                    
                    if total_size > 0:
                        progress = downloaded / total_size
                        download_progress.progress(min(progress, 1.0))
                        download_status.text(f"Downloaded: {downloaded / (1024*1024):.1f} MB / {total_size / (1024*1024):.1f} MB")
            
            download_status.text("Download complete! Uploading to S3 and extracting...")
            
            # Reset BytesIO position
            zip_bytes.seek(0)
            
            # Extract and upload each file to S3
            extract_count = 0
            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:
                file_list = zip_ref.namelist()
                extract_progress = st.progress(0)
                extract_status = st.empty()
                
                for idx, file_name in enumerate(file_list):
                    # Skip directories and hidden files
                    if file_name.endswith('/') or '/__MACOSX/' in file_name or file_name.startswith('.'):
                        continue
                    
                    # Only process image files
                    if not any(file_name.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png']):
                        continue
                    
                    extract_status.text(f"Extracting: {file_name}")
                    
                    # Read file from ZIP
                    file_data = zip_ref.read(file_name)
                    
                    # Upload to S3
                    s3_key = f"{s3_folder_key}{os.path.basename(file_name)}"
                    upload_to_s3_direct(file_data, S3_BUCKET, s3_key)
                    extract_count += 1
                    
                    extract_progress.progress((idx + 1) / len(file_list))
            
            extract_status.empty()
            extract_progress.empty()
            download_status.empty()
            download_progress.empty()
            
            if extract_count == 0:
                st.error("No images found in ZIP file")
                delete_s3_folder(S3_BUCKET, s3_folder_key)
                st.stop()
            
            # List all uploaded files from S3
            image_files = list_s3_files(S3_BUCKET, s3_folder_key)
            
            st.success(f"Found {len(image_files)} images uploaded to S3")
            
            # Store S3 folder key in session state for cleanup
            st.session_state['s3_folder_key'] = s3_folder_key
        
    except requests.RequestException as e:
        st.error(f"Failed to download from Dropbox: {e}")
        st.info("Make sure the link is a direct download link (ends with dl=1)")
        if s3_folder_key:
            delete_s3_folder(S3_BUCKET, s3_folder_key)
        st.stop()
    except zipfile.BadZipFile:
        st.error("Downloaded file is not a valid ZIP file")
        if s3_folder_key:
            delete_s3_folder(S3_BUCKET, s3_folder_key)
        st.stop()
    except Exception as e:
        st.error(f"Error: {e}")
        import traceback
        st.text(traceback.format_exc())
        if s3_folder_key:
            delete_s3_folder(S3_BUCKET, s3_folder_key)
        st.stop()
else:
    st.stop()

# Processing logic
if image_files:
    process = psutil.Process()
    initial_memory = process.memory_info().rss / (1024 * 1024)
    peak_memory = initial_memory
    
    if 'out_dir' not in st.session_state:
        st.session_state['out_dir'] = tempfile.mkdtemp()
    out_dir = st.session_state['out_dir']

    st.session_state['annotated_paths'] = []
    st.session_state['all_detections_records'] = []
    st.session_state['summary_records'] = []
    st.session_state['grouped_coords'] = defaultdict(list)
    st.session_state['max_counts'] = defaultdict(int)

    total_files = len(image_files)
    progress_bar = st.progress(0)
    progress_label = st.empty()
    status_text = st.empty()

    for idx, image_source in enumerate(image_files):
        p1 = None
        p2 = None
        tmp_path = None
        
        try:
            # Download from S3
            filename = os.path.basename(image_source)
            tmp_path = os.path.join(out_dir, f"s3_{idx}_{filename}")
            
            status_text.markdown(f"**Downloading {idx + 1}/{total_files}**: {filename}")
            download_from_s3(S3_BUCKET, image_source, tmp_path)
            
            image_path = tmp_path
            
            status_text.markdown(f"**Processing {idx + 1}/{total_files}**: {filename}")

            # Extract EXIF
            lat, lon = extract_gps_from_image(image_path)
            date, time = get_capture_date_time(image_path)

            if lat is None or lon is None:
                st.warning(f"No GPS in {filename}. Skipped.")
                if tmp_path and os.path.exists(tmp_path):
                    os.remove(tmp_path)
                continue

            # Resize for Roboflow limits
            p1, s1 = limit_resolution_to_temp(image_path)
            p2, s2 = progressive_resize_to_temp(p1)
            final_path = p2 or p1 or image_path
            scale = (s1 * s2) if p2 else (s1 if p1 else 1.0)

            # Run detection from local file
            detections = run_detection_from_local(final_path, conf_threshold, overlap_threshold)

            # Read for annotation BEFORE cleanup
            img = cv2.imread(final_path)
            if img is None:
                raise RuntimeError(f"Could not read {filename}")

            # NOW cleanup resize temps and downloaded file (after reading)
            for p in (p2, p1, tmp_path):
                if p and p != image_path and os.path.exists(p):
                    try:
                        os.remove(p)
                    except:
                        pass

            gsd = compute_gsd(img.shape[1])

            annotator = sv.BoxAnnotator()
            texts = [f"seal {c*100:.1f}%" for c in detections.confidence]
            labeled = annotator.annotate(scene=img.copy(), detections=detections, labels=texts) \
                if "labels" in inspect.signature(annotator.annotate).parameters \
                else annotator.annotate(scene=img.copy(), detections=detections)

            ann_tmp = tempfile.NamedTemporaryFile(suffix='.jpg', delete=False)
            ann_path = ann_tmp.name
            ann_tmp.close()
            cv2.imwrite(ann_path, labeled)
            st.session_state['annotated_paths'].append(ann_path)

            # Scale boxes back to original coordinates
            scaled_boxes = [[b[0]*scale, b[1]*scale, b[2]*scale, b[3]*scale] for b in detections.xyxy]

            for seal_idx, (box, conf) in enumerate(zip(scaled_boxes, detections.confidence), start=1):
                st.session_state['all_detections_records'].append({
                    "image_name": filename,
                    "seal_id": seal_idx,
                    "x_min": float(box[0]),
                    "y_min": float(box[1]),
                    "x_max": float(box[2]),
                    "y_max": float(box[3]),
                    "confidence": float(conf)
                })

            location = get_location_name(lat)
            group_key = (location, date)

            for box in detections.xyxy:
                xc = (box[0] + box[2]) / 2
                yc = (box[1] + box[3]) / 2
                dx_m = (xc - img.shape[1] / 2) * gsd
                dy_m = (yc - img.shape[0] / 2) * gsd
                lat_off = dy_m / 111320
                lon_off = dx_m / (111320 * cos(radians(lat)))
                st.session_state['grouped_coords'][group_key].append((lat + lat_off, lon + lon_off))

            st.session_state['summary_records'].append({
                "image_name": filename,
                "pinniped_count": len(scaled_boxes),
                "latitude": lat,
                "longitude": lon,
                "location": location,
                "date": date,
                "time": time
            })

            key = (location, date)
            current_max = st.session_state['max_counts'].get(key, 0)
            st.session_state['max_counts'][key] = max(current_max, len(scaled_boxes))

            del img, labeled
            gc.collect()

            current_memory = process.memory_info().rss / (1024 * 1024)
            peak_memory = max(peak_memory, current_memory)

            progress_bar.progress((idx + 1) / total_files)
            progress_label.text(f"{idx + 1}/{total_files} complete")

        except Exception as e:
            st.error(f"Error processing {filename if 'filename' in locals() else 'image'}: {e}")
            import traceback
            st.text(traceback.format_exc())
            
            # Cleanup on error
            for p in (p2, p1, tmp_path):
                if p and os.path.exists(p):
                    try:
                        os.remove(p)
                    except:
                        pass
            gc.collect()

    final_memory = process.memory_info().rss / (1024 * 1024)
    memory_used = final_memory - initial_memory
    
    # Store memory stats in session state
    st.session_state['peak_memory'] = peak_memory
    st.session_state['final_memory'] = final_memory
    st.session_state['memory_used'] = memory_used

    status_text.markdown(f"**‚úÖ Complete! {len(st.session_state['summary_records'])}/{total_files} processed**")

    # Cleanup S3 folder if it exists
    if 's3_folder_key' in st.session_state:
        try:
            with st.spinner("Cleaning up S3..."):
                delete_s3_folder(S3_BUCKET, st.session_state['s3_folder_key'])
            del st.session_state['s3_folder_key']
        except Exception as e:
            st.warning(f"Could not cleanup S3: {e}")

    st.markdown("### üìä Memory Usage")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Peak RAM", f"{peak_memory:.1f} MB")
    with col2:
        st.metric("Final RAM", f"{final_memory:.1f} MB")
    with col3:
        st.metric("RAM Increase", f"{memory_used:.1f} MB")

    st.markdown("### üì∑ Annotated Images")
    for idx, ap in enumerate(st.session_state.get('annotated_paths', [])):
        try:
            if idx < len(st.session_state['summary_records']):
                filename = st.session_state['summary_records'][idx]['image_name']
            else:
                filename = f"Image {idx + 1}"
            
            st.image(cv2.cvtColor(cv2.imread(ap), cv2.COLOR_BGR2RGB))
            st.markdown(f"<p style='text-align: center; color: gray; font-size: 12px; margin-top: -15px;'>Img: {filename}</p>", unsafe_allow_html=True)
        except:
            st.image(ap)
            if idx < len(st.session_state['summary_records']):
                filename = st.session_state['summary_records'][idx]['image_name']
                st.markdown(f"<p style='text-align: center; color: gray; font-size: 12px; margin-top: -15px;'>Img: {filename}</p>", unsafe_allow_html=True)

    folder_summary_records = compute_unique_counts(
        st.session_state['grouped_coords'],
        st.session_state['max_counts']
    )
    display_and_download_summary(
        st.session_state['summary_records'],
        folder_summary_records,
        st.session_state['all_detections_records']
    )

    st.info("""
    ### üìã What's Next?
    
    **1. Verify Results** üì∏  
    Review the annotated images above. Each detection is marked with a confidence score. Adjust the thresholds above and reprocess if needed.
    
    **2. Download Data** üíæ  
    Two CSV files are available for download:
    - **Per-Image**: Individual detections with bounding boxes and confidence scores
    - **Per-Location**: Aggregated counts by location and date
    
    **3. Edit & Upload** ‚úèÔ∏è  
    Make any necessary corrections to the downloaded CSV files and upload them to ODK using the provided link.
    """)